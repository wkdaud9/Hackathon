import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client
from flask import Blueprint, jsonify
import re
from dotenv import load_dotenv

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()

# 'scraper'ë¼ëŠ” ì´ë¦„ì˜ Blueprintë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
bp = Blueprint('scraper', __name__, url_prefix='/scrape')

# --- Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# --- í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ URL ì •ì˜ ---
NEWS_CATEGORIES = {
    'home': 'https://news.daum.net/',
    'economy': 'https://news.daum.net/economic',
    'politics': 'https://news.daum.net/politics',
    'international': 'https://news.daum.net/foreign',
    'it': 'https://news.daum.net/digital'
}

# views/scraper_views.py -> get_article_details í•¨ìˆ˜ ìˆ˜ì •

def get_article_details(url, category):
    """(ìˆ˜ì •) ì¹´ë“œë‰´ìŠ¤ ì¸ë„¤ì¼ ì¶”ì¶œ ë¡œì§ì´ ì¶”ê°€ëœ í•¨ìˆ˜"""
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        article_id = url.split('/')[-1]
        title = soup.find("h3", class_="tit_view").get_text(strip=True) if soup.find("h3", class_="tit_view") else "ì œëª© ì—†ìŒ"
        
        thumbnail_url = "ì¸ë„¤ì¼ ì—†ìŒ"
        
        # â–¼â–¼â–¼ ì—¬ê¸°ê°€ í•µì‹¬ ìˆ˜ì • ë¶€ë¶„ì…ë‹ˆë‹¤ â–¼â–¼â–¼
        # Plan A: meta íƒœê·¸ ë¨¼ì € ì‹œë„ (ê°€ì¥ ì¼ë°˜ì )
        thumbnail_tag = soup.find("meta", property="og:image")
        if thumbnail_tag:
            thumbnail_url = thumbnail_tag['content']
        else:
            # Plan B: ë³¸ë¬¸ ëŒ€í‘œ ì´ë¯¸ì§€ ì‹œë„
            img_tag = soup.find("img", class_="thumb_g_article")
            if img_tag:
                thumbnail_url = img_tag['src']
            else:
                # Plan C: ì¹´ë“œë‰´ìŠ¤/í¬í† ë‰´ìŠ¤ í˜•ì‹ì˜ ì´ë¯¸ì§€ ì‹œë„
                photo_img_tag = soup.select_one(".gallery_view img")
                if photo_img_tag:
                    thumbnail_url = photo_img_tag['src']

    
        # DBì— ì €ì¥í•˜ì§€ ì•Šì„ ì •ë³´ëŠ” ë°˜í™˜ ê°ì²´ì—ì„œ ì œì™¸
        return {
            'article_id': article_id,
            'title': title,
            'thumbnail': thumbnail_url,
            'category': category,
            'url': url,
        }
    except Exception as e:
        print(f"Error scraping {url}: {e}")
        return None

def get_news_urls(list_url):
    """ì¤‘ë³µì„ ì œê±°í•˜ì—¬ ë‰´ìŠ¤ ê¸°ì‚¬ URLì„ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜"""
    response = requests.get(list_url, headers={'User-Agent': 'Mozilla/5.0'})
    soup = BeautifulSoup(response.text, "html.parser")
    
    unique_urls = set()
    link_selectors = ["a.item_newsheadline2", "a.link_txt", "a.link_mainnews"]
    
    article_links = soup.select(', '.join(link_selectors))
    
    for link in article_links:
        href = link.get('href')
        if href and href.startswith('https://v.daum.net/v/'):
            unique_urls.add(href)

    return list(unique_urls)[:10]
@bp.route('/start')
def start_scraping():
    """/scrape/start ì£¼ì†Œë¡œ ì ‘ì†í•˜ë©´ ëª¨ë“  ì¹´í…Œê³ ë¦¬ì˜ ë‰´ìŠ¤ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜"""
    all_scraped_data = []
    
    for category, url in NEWS_CATEGORIES.items():
        print(f"--- ğŸ“° '{category}' ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì‹œì‘ ---")
        
        target_urls = get_news_urls(url)
        print(f"âœ… URL {len(target_urls)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ.")
        
        for article_url in target_urls:
            details = get_article_details(article_url, category)
            if details and details['title'] != "ì œëª© ì—†ìŒ":
                all_scraped_data.append(details)

    if all_scraped_data:
        # DBì— ì €ì¥í•˜ê¸° ì „, ì „ì²´ ë°ì´í„°ì—ì„œ ì¤‘ë³µ ê¸°ì‚¬ë¥¼ ìµœì¢…ì ìœ¼ë¡œ ì œê±°í•©ë‹ˆë‹¤.
        unique_articles = {article['article_id']: article for article in all_scraped_data}
        final_unique_data = list(unique_articles.values())
        
        print(f"\nğŸ’¾ ìˆ˜ì§‘ëœ ì „ì²´ ë‰´ìŠ¤ {len(all_scraped_data)}ê°œ ì¤‘, ì¤‘ë³µì„ ì œì™¸í•œ {len(final_unique_data)}ê°œë¥¼ Supabase DBì— ì €ì¥í•©ë‹ˆë‹¤...")
        try:
            # ì¤‘ë³µì´ ì œê±°ëœ final_unique_dataë¥¼ DBì— ì €ì¥
            supabase.table('articles').upsert(final_unique_data).execute()
            print("âœ… ë°ì´í„° ì €ì¥(ë˜ëŠ” ì—…ë°ì´íŠ¸) ì„±ê³µ!")
            return jsonify({'status': 'success', 'message': f'ì´ {len(final_unique_data)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.'})
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return jsonify({'status': 'error', 'message': str(e)}), 500
            
    return jsonify({'status': 'no_data', 'message': 'ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.'})

@bp.route('/refetch/<article_id>')
def refetch_article(article_id):
    """ê¸°ì‚¬ IDë¥¼ ë°›ì•„ í•´ë‹¹ ê¸°ì‚¬ë§Œ ë‹¤ì‹œ í¬ë¡¤ë§í•˜ê³  DBì— ì—…ë°ì´íŠ¸í•˜ëŠ” API"""
    if not article_id:
        return jsonify({'status': 'error', 'message': 'ê¸°ì‚¬ IDê°€ í•„ìš”í•©ë‹ˆë‹¤.'}), 400

    target_url = f"https://v.daum.net/v/{article_id}"
    print(f"--- ğŸ”„ íŠ¹ì • ê¸°ì‚¬ ì¬ìˆ˜ì§‘ ì‹œì‘: {target_url} ---")
    
    # ì¬ìˆ˜ì§‘ ì‹œ ì¹´í…Œê³ ë¦¬ëŠ” 'manual'ë¡œ ì§€ì •í•˜ê±°ë‚˜, ê¸°ì¡´ ì¹´í…Œê³ ë¦¬ë¥¼ DBì—ì„œ ì¡°íšŒí•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ 'manual'ë¡œ ì§€ì •í•©ë‹ˆë‹¤.
    details = get_article_details(target_url, 'manual_refetch')

    if details:
        print(f"âœ… ì¬ìˆ˜ì§‘ ì„±ê³µ. DBì— ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤...")
        try:
            # upsertë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì¡´ ë°ì´í„°ë¥¼ ë®ì–´ì”ë‹ˆë‹¤.
            supabase.table('articles').upsert(details).execute()
            print("âœ… ë°ì´í„° ì—…ë°ì´íŠ¸ ì„±ê³µ!")
            return jsonify({'status': 'success', 'message': f"ê¸°ì‚¬({article_id})ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¬ìˆ˜ì§‘í•˜ê³  ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤."})
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
            return jsonify({'status': 'error', 'message': str(e)}), 500
            
    return jsonify({'status': 'error', 'message': 'í•´ë‹¹ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'}), 500
