import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client
from flask import Blueprint, jsonify
import re
from dotenv import load_dotenv # .env íŒŒì¼ì„ ì½ê¸° ìœ„í•´ ì¶”ê°€

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()

bp = Blueprint('scraper', __name__, url_prefix='/scrape')

# --- Supabase ì ‘ì† ì •ë³´ ìˆ˜ì • ---
# os.getenv() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ .env íŒŒì¼ì˜ ê°’ì„ ì½ì–´ì˜µë‹ˆë‹¤.
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

# (ë‚˜ë¨¸ì§€ ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼í•©ë‹ˆë‹¤)
# ... get_article_details, get_news_urls, start_scraping í•¨ìˆ˜ë“¤ ...
def get_article_details(url):
    # (ì´ì „ ì½”ë“œì™€ ë™ì¼)
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        article_id = url.split('/')[-1]
        title_tag = soup.find("h3", class_="tit_view")
        title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"
        content_tag = soup.find("div", class_="article_view")
        content = content_tag.get_text() if content_tag else "ë³¸ë¬¸ ì—†ìŒ"
        content = re.sub(r'[\s\n\t]+', ' ', content).strip()
        thumbnail_url = "ì¸ë„¤-ì¼ ì—†ìŒ"
        meta_tag = soup.find("meta", property="og:image")
        if meta_tag:
            thumbnail_url = meta_tag['content']
        else:
            img_tag = soup.find("img", class_="thumb_g_article")
            if img_tag:
                thumbnail_url = img_tag['src']
        return { 'article_id': article_id, 'title': title, 'content': content, 'thumbnail': thumbnail_url, }
    except Exception as e:
        print(f"Error scraping {url}: {e}")
        return None

def get_news_urls(list_url):
    # (ì´ì „ ì½”ë“œì™€ ë™ì¼)
    response = requests.get(list_url, headers={'User-Agent': 'Mozilla/5.0'})
    soup = BeautifulSoup(response.text, "html.parser")
    news_list = []
    article_links = soup.select("a.item_newsheadline2")
    for link in article_links[:10]:
        news_list.append(link['href'])
    return news_list

@bp.route('/start')
def start_scraping():
    """/scrape/start ì£¼ì†Œë¡œ ì ‘ì†í•˜ë©´ í¬ë¡¤ë§ì„ ì‹œì‘í•˜ê³  DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    DAUM_NEWS_URL = "https://news.daum.net/"
    print("ğŸ“° Daum ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ìµœì‹  ê¸°ì‚¬ URL 10ê°œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
    target_urls = get_news_urls(DAUM_NEWS_URL)
    print(f"âœ… URL ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(target_urls)}ê°œì˜ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.\n")
    final_news_data = []
    for url in target_urls:
        details = get_article_details(url)
        if details and details['title'] != "ì œëª© ì—†ìŒ":
            final_news_data.append(details)

    if final_news_data:
        print(f"ğŸ’¾ ìˆ˜ì§‘ëœ {len(final_news_data)}ê°œì˜ ë‰´ìŠ¤ë¥¼ Supabase DBì— ì €ì¥í•©ë‹ˆë‹¤...")
        try:
            supabase.table('articles').upsert(final_news_data).execute()
            print("âœ… ë°ì´í„° ì €ì¥(ë˜ëŠ” ì—…ë°ì´íŠ¸) ì„±ê³µ!")
            return jsonify({'status': 'success', 'message': f'{len(final_news_data)}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.'})
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return jsonify({'status': 'error', 'message': str(e)}), 500
            
    return jsonify({'status': 'no_data', 'message': 'ì €ì¥í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.'})