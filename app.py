import os
import requests
from bs4 import BeautifulSoup
from supabase import create_client, Client

# --- Supabase ì ‘ì† ì •ë³´ ---
SUPABASE_URL = "https://vuyiczcaweraqncjxllz.supabase.co"
SUPABASE_KEY = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InZ1eWljemNhd2VyYXFuY2p4bGx6Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1ODU0NDg2MiwiZXhwIjoyMDc0MTIwODYyfQ.jUOdJInNFQbGYMU2CCX4dzqu4FpWVV9B_0z0d076IWg"
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

def get_article_details(url):
    """(ìµœì¢…) ê¸°ì‚¬ URLì—ì„œ ê³ ìœ  IDë¥¼ ì¶”ì¶œí•˜ê³  ìƒì„¸ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # â–¼â–¼â–¼ ê³ ìœ  ID ì¶”ì¶œ ë¡œì§ ì¶”ê°€ â–¼â–¼â–¼
        # URL("https://v.daum.net/v/20250922213512790")ì—ì„œ ë§ˆì§€ë§‰ ìˆ«ì ë¶€ë¶„ì„ IDë¡œ ì‚¬ìš©
        article_id = url.split('/')[-1]

        title_tag = soup.find("h3", class_="tit_view")
        title = title_tag.get_text(strip=True) if title_tag else "ì œëª© ì—†ìŒ"

        content_tag = soup.find("div", class_="article_view")
        content = content_tag.get_text(strip=True) if content_tag else "ë³¸ë¬¸ ì—†ìŒ"

        thumbnail_url = "ì¸ë„¤ì¼ ì—†ìŒ"
        meta_tag = soup.find("meta", property="og:image")
        if meta_tag:
            thumbnail_url = meta_tag['content']
        else:
            img_tag = soup.find("img", class_="thumb_g_article")
            if img_tag:
                thumbnail_url = img_tag['src']

        # ë°˜í™˜ ë°ì´í„°ì— article_id ì¶”ê°€
        return {
            'article_id': article_id,
            'title': title,
            'content': content,
            'thumbnail': thumbnail_url,
        }
    except Exception as e:
        print(f"Error scraping {url}: {e}")
        return None

def get_news_urls(list_url):
    """ëª©ë¡ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ URL 10ê°œë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜"""
    response = requests.get(list_url, headers={'User-Agent': 'Mozilla/5.0'})
    soup = BeautifulSoup(response.text, "html.parser")
    
    news_list = []
    article_links = soup.select("a.item_newsheadline2")
    
    for link in article_links[:10]:
        news_list.append(link['href'])
        
    return news_list

# --- ë©”ì¸ ì‹¤í–‰ ë¶€ë¶„ ---
if __name__ == "__main__":
    DAUM_NEWS_URL = "https://news.daum.net/"
    
    print("ğŸ“° Daum ë‰´ìŠ¤ ëª©ë¡ì—ì„œ ìµœì‹  ê¸°ì‚¬ URL 10ê°œë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤...")
    target_urls = get_news_urls(DAUM_NEWS_URL)
    
    print(f"âœ… URL ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(target_urls)}ê°œì˜ ê¸°ì‚¬ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.\n")
    
    final_news_data = []
    for url in target_urls:
        details = get_article_details(url)
        if details and details['title'] != "ì œëª© ì—†ìŒ":
            final_news_data.append(details)

    if final_news_data:
        print(f"ğŸ’¾ ìˆ˜ì§‘ëœ {len(final_news_data)}ê°œì˜ ë‰´ìŠ¤ë¥¼ Supabase DBì— ì €ì¥í•©ë‹ˆë‹¤...")
        try:
            # 'articles' í…Œì´ë¸”ì— ë°ì´í„°ë¥¼ ì‚½ì… (upsert ì‚¬ìš©)
            # upsert: ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì… (ì¤‘ë³µ ë°©ì§€)
            data, count = supabase.table('articles').upsert(final_news_data).execute()
            print("âœ… ë°ì´í„° ì €ì¥(ë˜ëŠ” ì—…ë°ì´íŠ¸) ì„±ê³µ!")
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")